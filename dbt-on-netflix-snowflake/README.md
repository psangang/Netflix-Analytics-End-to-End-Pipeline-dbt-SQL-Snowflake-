ğŸš€ Project Overview

This project demonstrates a complete end-to-end modern data pipeline using the Netflix dataset. It showcases how raw JSON/CSV data is transformed into clean, analytics-ready fact and dimension tables using dbt, Snowflake, and best practices inspired by modern data engineering.

The goal of this project is to replicate how real companies structure their data pipelines â€” following modular SQL, semantic modeling, data quality testing, and automated transformations.

ğŸŒ High-Level Architecture

           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚     Raw Netflix Data     â”‚
           â”‚   (CSV / JSON files)     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚     Snowflake (RAW)      â”‚
           â”‚  Loaded via Snowflake UI â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             dbt                â”‚
        â”‚  - Staging Models (STG)        â”‚
        â”‚  - Intermediate Models (INT)   â”‚
        â”‚  - Fact & Dim Tables (MARTS)   â”‚
        â”‚  - Generic & Custom Tests       â”‚
        â”‚  - Documentation & Lineage      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚      Analytics-Ready Dataset        â”‚
     â”‚  (Fact tables, dim tables, KPIs)    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     ğŸ“ Project Structure


    dbt-on-netflix-snowflake/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”œâ”€â”€ marts/
â”‚   â”œâ”€â”€ tests/
â”‚
â”œâ”€â”€ macros/
â”‚
â”œâ”€â”€ seeds/
â”‚
â”œâ”€â”€ snapshots/
â”‚
â””â”€â”€ dbt_project.yml

Each folder represents a clean layer in the analytics workflow â€” similar to Medallion Architecture (Bronze â†’ Silver â†’ Gold).

â¸»

ğŸ” Detailed Project Walkthrough

1. Raw Data Loading
	â€¢	Netflix data is uploaded into Snowflake (RAW schema).
	â€¢	Source tables are registered using sources.yml.

â¸»

2. Staging Layer (STG)

Purpose:
	â€¢	Clean raw data
	â€¢	Standardize column names
	â€¢	Fix datatypes
	â€¢	Remove duplicates

Example:
select
    id,
    title,
    type,
    release_year,
    trim(description) as description
from {{ source('raw', 'netflix_titles') }}

3. Intermediate Layer (INT)

Purpose:
	â€¢	Apply business rules
	â€¢	Joins across datasets
	â€¢	Add calculated fields

â¸»

4. Marts Layer (FACT & DIM)

Purpose:
	â€¢	Final analytics-ready tables
	â€¢	Fact tables for metrics
	â€¢	Dimension tables for reporting

â¸»

5. Data Quality Tests

Includes:
	â€¢	Generic tests (unique, not_null, accepted_values)
	â€¢	Custom tests (e.g., check nulls across all columns)

Example:
tests:
  - unique:
      column_name: id
  - not_null:
      column_name: title

      6. Documentation & Lineage
	â€¢	Run dbt docs generate
	â€¢	View with dbt docs serve
	â€¢	Interactive lineage graph is created automatically

â¸»

ğŸ› ï¸ How to Run This Project

1. Clone Repo
git clone https://github.com/psangang/Netflix-Analytics-End-to-End-Pipeline-dbt-SQL-Snowflake-.git
cd dbt-on-netflix-snowflake

2. Install dbt
Mac:
brew update
brew install dbt-snowflake

3. Set Up Your profiles.yml
Example Snowflake config:
netflix_dbt:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: <account>
      user: <user>
      password: <pwd>
      role: transform
      warehouse: compute_wh
      database: netflix
      schema: analytics
      threads: 4

4. Run Models
    dbt run

5. Run Tests
    dbt test

6. View Documentation
    dbt docs generate
    dbt docs serve

ğŸ¯ What This Project Demonstrates

âœ” Ability to build end-to-end pipelines
âœ” Strong SQL transformation skills
âœ” dbt best practices (staging â†’ marts, modular modeling)
âœ” Snowflake cloud data warehousing
âœ” Implementing data quality and lineage
âœ” Industry-level analytics engineering patterns

â¸»

ğŸ§  Learning Roadmap (If Youâ€™re New)

Step 1 â€” Learn dbt Fundamentals
	â€¢	Models
	â€¢	Sources
	â€¢	Tests
	â€¢	Documentation
	â€¢	Jinja macros

Step 2 â€” Learn Snowflake Basics
	â€¢	Warehouses
	â€¢	Databases & schemas
	â€¢	Virtual compute
	â€¢	SQL features

Step 3 â€” Build More End-to-End Projects



